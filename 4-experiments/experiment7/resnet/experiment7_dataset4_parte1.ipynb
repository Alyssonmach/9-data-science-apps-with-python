{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment7-dataset4-parte1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMXBldW4hnUum6bL5TLHCV8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhTSVzsrOkc2"
      },
      "source": [
        "# Experimento 7\n",
        "***\n",
        "- Conjunto de Dados: VinBigData\n",
        "- Testando a predição por multi-classe\n",
        "- Arquitetura utilizada: ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brX85Uc1OyPC"
      },
      "source": [
        "### Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBMyCX5zOXXg"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from keras.applications.resnet import ResNet152\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgGyQ6JaO8Is"
      },
      "source": [
        "### Pré-processamento nos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uo3LvGPO-UI"
      },
      "source": [
        "# lendo os dados de um arquivo csv\n",
        "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
        "# criando uma coluna com os caminhos relativos as imagens\n",
        "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata/train/' + dataframe.image_id + '.jpg'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzWW5oxMPMTU",
        "outputId": "eff18d62-0441-4bbc-b45a-2e396d5b4f9b"
      },
      "source": [
        "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de imagens disponíveis: 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vorowA3WPNIz",
        "outputId": "dd61cfea-6b98-4213-96e5-f163515e2658"
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Aortic enlargement     7162\n",
              "Cardiomegaly           5427\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Other lesion           2203\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5UL8Up3PRwW"
      },
      "source": [
        "# removendo os casos não relativos a distúrbios pulmonares\n",
        "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
        "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
        "dataframe = dataframe[dataframe.class_name != 'Other lesion']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xsRVeUoPT6m",
        "outputId": "c1475731-68e7-49ac-9fcc-8937bb6467e3"
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfQwdlJEPWG9",
        "outputId": "1e6db22a-c732-4880-c7d5-9ce7a8866a86"
      },
      "source": [
        "# separando os casos rotulados como normais e anormais\n",
        "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
        "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
        "\n",
        "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de dados após a filtração: 13952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUges0dkPcO8"
      },
      "source": [
        "# removendo as imagens repetidas\n",
        "normal_data = normal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "abnormal_data = abnormal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "\n",
        "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
        "normal_data['target'] = 'normal'\n",
        "abnormal_data['target'] = 'abnormal'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYKem_dQPd9c",
        "outputId": "922b182a-5841-4871-e7cf-80060191e3d2"
      },
      "source": [
        "# visualizando a quantidade de exemplos após a remoção de duplicatas\n",
        "abnormal_data['class_name'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pleural thickening    901\n",
              "Pulmonary fibrosis    742\n",
              "Lung Opacity          427\n",
              "Nodule/Mass           339\n",
              "Pleural effusion      328\n",
              "Calcification         167\n",
              "Infiltration          163\n",
              "ILD                   152\n",
              "Consolidation          59\n",
              "Atelectasis            41\n",
              "Pneumothorax           27\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSK8Ml4UPgQO",
        "outputId": "48b21ef9-b9b5-48f8-8e74-14e4860376c5"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 10606\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Udc7SyqPh6D"
      },
      "source": [
        "# removendo 69% dos casos normais para balancear os dados\n",
        "normal, _ = train_test_split(normal_data, test_size = 0.90, random_state = 42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96yJqGd-Pjcz",
        "outputId": "e25b77e3-2b3e-466b-d070-7ae8c85dbfa0"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 1060\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4mjI40nPm-T"
      },
      "source": [
        "# concatenando os dataframes de casos normais e anormais\n",
        "full_data = pd.concat([normal, abnormal_data])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkBI4DwhPuON"
      },
      "source": [
        "# misturando todos os dados do dataframe e reiniciando os valores dos índices \n",
        "full_data = full_data.sample(frac = 1, axis = 0, random_state = 42).reset_index(drop=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUlYJAtwPvsE"
      },
      "source": [
        "# separando os dados de treinamento e de teste\n",
        "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
        "                                     test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOEeEnq2PwQf"
      },
      "source": [
        "# separando os dados de validação dos dados de treinamento\n",
        "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
        "                                           test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fKGPXnzPzXz",
        "outputId": "c8ba676f-5671-419e-f9f9-ab92748b2289"
      },
      "source": [
        "# visualizando a quantidade de dados\n",
        "print('quantidade de imagens de treinamento:', len(train_df['image_path']))\n",
        "print('quantidade de rótulos de treinamento:', len(train_df['class_name']))\n",
        "print('quantidade de imagens de teste:', len(test_df['image_path']))\n",
        "print('quantidade de rótulos de teste:', len(test_df['class_name']))\n",
        "print('quantidade de imagens de validação:', len(validation_df['image_path']))\n",
        "print('quantidade de rótulos de validação:', len(validation_df['class_name']))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de imagens de treinamento: 2819\n",
            "quantidade de rótulos de treinamento: 2819\n",
            "quantidade de imagens de teste: 882\n",
            "quantidade de rótulos de teste: 882\n",
            "quantidade de imagens de validação: 705\n",
            "quantidade de rótulos de validação: 705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S6neb8LP1gY"
      },
      "source": [
        "### Aplicando Mudança de Escala Típica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7jzNVzkP5e8",
        "outputId": "96ab31d9-0811-40f3-8575-0e0d5b8d6d05"
      },
      "source": [
        "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
        "image_generator = ImageDataGenerator(rescale = 1./255.,\n",
        "                                     rotation_range = 10, zoom_range = 0.2)\n",
        "\n",
        "# criando o gerador de imagens de treinamento \n",
        "train_generator = image_generator.flow_from_dataframe(\n",
        "                                                      dataframe = train_df,\n",
        "                                                      directory = '',\n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "# normalizando as imagens de teste e validação\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
        "\n",
        "# criando o gerador de imagens de validação \n",
        "valid_generator = test_datagen.flow_from_dataframe(\n",
        "                                                      dataframe = validation_df,\n",
        "                                                      directory = '.', \n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "                                                  dataframe = test_df, \n",
        "                                                  directory = '.',\n",
        "                                                  x_col = 'image_path',\n",
        "                                                  y_col = 'class_name',\n",
        "                                                  batch_size = 32,\n",
        "                                                  seed = 42,\n",
        "                                                  shuffle = True,\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  target_size = (256, 256))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2819 validated image filenames belonging to 12 classes.\n",
            "Found 705 validated image filenames belonging to 12 classes.\n",
            "Found 882 validated image filenames belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTFTB5WCP8Oj",
        "outputId": "f3ac7ddf-9650-42a3-bd70-2f5d131d74dc"
      },
      "source": [
        "# visualizando a ordem númerica das classes\n",
        "train_generator.class_indices"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Atelectasis': 0,\n",
              " 'Calcification': 1,\n",
              " 'Consolidation': 2,\n",
              " 'ILD': 3,\n",
              " 'Infiltration': 4,\n",
              " 'Lung Opacity': 5,\n",
              " 'No finding': 6,\n",
              " 'Nodule/Mass': 7,\n",
              " 'Pleural effusion': 8,\n",
              " 'Pleural thickening': 9,\n",
              " 'Pneumothorax': 10,\n",
              " 'Pulmonary fibrosis': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkcq9Qu0P9z-"
      },
      "source": [
        "### Preparando a rede neural convolucional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bb9DmNAP9c3",
        "outputId": "2e79d26c-5129-452e-d7af-4dd6974674da"
      },
      "source": [
        "# realizando a transferência de aprendizagem com a rede ResNet\n",
        "base_model = ResNet152(input_shape = (256, 256, 3), weights = 'imagenet', include_top = False)\n",
        "\n",
        "# pegando a última camada da DenseNet\n",
        "x = base_model.output\n",
        "# adicionando uma camada de Global Average Pooling\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "# adicionando uma camada densa com 512 neurônios\n",
        "x = layers.Dense(units = 512, activation = tf.nn.relu)(x)     \n",
        "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
        "x = layers.Dense(units = 256, activation = tf.nn.relu)(x) \n",
        "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
        "x = layers.Dropout(rate = 0.2)(x)\n",
        "# adicionando a camada de saída da rede \n",
        "predictions = layers.Dense(units = 12, activation = tf.nn.softmax)(x)\n",
        "\n",
        "# concatenando a rede importada com a rede definida (ajuste fino)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234700800/234698864 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsJLkdV1QWH-"
      },
      "source": [
        "# definindo as flags iniciais  \n",
        "model.trainable = True\n",
        "set_trainable = False\n",
        "\n",
        "# para a arquitetura DenseNet, a rede será retreinada a partir da camada 'conv5_block1_1_conv'\n",
        "for layer in model.layers:\n",
        "    if layer.name == 'conv5_block1_1_conv':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7MDhvaQYo8"
      },
      "source": [
        "# compilando a rede \n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJM82kafQbfw"
      },
      "source": [
        "# definindo o caminho pelo qual os pesos serão armazenados \n",
        "filepath = \"/content/drive/MyDrive/experimentos/v2.0-exp7-ds4/resnet/transferlearning_weights.hdf5\"\n",
        "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, \n",
        "                             mode = 'max')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcfgoV7pQtx_"
      },
      "source": [
        "# definindo um array de callbacks\n",
        "callbacks = [checkpoint]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4sQyVh4Qxyw",
        "outputId": "95285ffb-19d3-40ad-a96e-539b30dbedc3"
      },
      "source": [
        "# treinando a rede neural convolucional\n",
        "history = model.fit_generator(train_generator, steps_per_epoch = 2819 // 32, \n",
        "                              validation_data = valid_generator, validation_steps = 705 // 32,\n",
        "                              callbacks = callbacks, epochs = 50, use_multiprocessing = True,\n",
        "                              workers = 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.1812 - acc: 0.2277WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 445s 4s/step - loss: 2.1808 - acc: 0.2279 - val_loss: 2.2930 - val_acc: 0.2415\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24148, saving model to /content/drive/MyDrive/experimentos/v2.0-exp7-ds4/resnet/transferlearning_weights.hdf5\n",
            "Epoch 2/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.0499 - acc: 0.2837WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 966ms/step - loss: 2.0497 - acc: 0.2838 - val_loss: 2.1107 - val_acc: 0.2614\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.24148 to 0.26136, saving model to /content/drive/MyDrive/experimentos/v2.0-exp7-ds4/resnet/transferlearning_weights.hdf5\n",
            "Epoch 3/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.0356 - acc: 0.2799WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 966ms/step - loss: 2.0352 - acc: 0.2801 - val_loss: 2.1484 - val_acc: 0.2443\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.26136\n",
            "Epoch 4/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.9827 - acc: 0.3146WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 964ms/step - loss: 1.9825 - acc: 0.3147 - val_loss: 2.3563 - val_acc: 0.1875\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.26136\n",
            "Epoch 5/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.9466 - acc: 0.3389WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 967ms/step - loss: 1.9463 - acc: 0.3388 - val_loss: 2.4242 - val_acc: 0.1378\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.26136\n",
            "Epoch 6/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.9235 - acc: 0.3161WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 970ms/step - loss: 1.9234 - acc: 0.3162 - val_loss: 4.0478 - val_acc: 0.0852\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.26136\n",
            "Epoch 7/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8761 - acc: 0.3540WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 962ms/step - loss: 1.8763 - acc: 0.3539 - val_loss: 3.6257 - val_acc: 0.0881\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.26136\n",
            "Epoch 8/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8803 - acc: 0.3377WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 965ms/step - loss: 1.8802 - acc: 0.3378 - val_loss: 2.6435 - val_acc: 0.1378\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.26136\n",
            "Epoch 9/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8395 - acc: 0.3626WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 966ms/step - loss: 1.8398 - acc: 0.3624 - val_loss: 3.8042 - val_acc: 0.1108\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.26136\n",
            "Epoch 10/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8530 - acc: 0.3566WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 970ms/step - loss: 1.8531 - acc: 0.3566 - val_loss: 3.8070 - val_acc: 0.1321\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.26136\n",
            "Epoch 11/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8390 - acc: 0.3635WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 964ms/step - loss: 1.8392 - acc: 0.3635 - val_loss: 7.6652 - val_acc: 0.1023\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.26136\n",
            "Epoch 12/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8638 - acc: 0.3620WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 980ms/step - loss: 1.8637 - acc: 0.3619 - val_loss: 5.4629 - val_acc: 0.0866\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.26136\n",
            "Epoch 13/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8309 - acc: 0.3533WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 970ms/step - loss: 1.8308 - acc: 0.3534 - val_loss: 3.0179 - val_acc: 0.1207\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.26136\n",
            "Epoch 14/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8661 - acc: 0.3524WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 967ms/step - loss: 1.8657 - acc: 0.3526 - val_loss: 4.5964 - val_acc: 0.1080\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.26136\n",
            "Epoch 15/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8499 - acc: 0.3621WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 970ms/step - loss: 1.8497 - acc: 0.3622 - val_loss: 12.9625 - val_acc: 0.2401\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.26136\n",
            "Epoch 16/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8135 - acc: 0.3540WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 973ms/step - loss: 1.8136 - acc: 0.3541 - val_loss: 23.6389 - val_acc: 0.2401\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.26136\n",
            "Epoch 17/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7951 - acc: 0.3774WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 965ms/step - loss: 1.7953 - acc: 0.3774 - val_loss: 14.3917 - val_acc: 0.2415\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.26136\n",
            "Epoch 18/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8323 - acc: 0.3649WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 974ms/step - loss: 1.8323 - acc: 0.3648 - val_loss: 2.3744 - val_acc: 0.3239\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.26136 to 0.32386, saving model to /content/drive/MyDrive/experimentos/v2.0-exp7-ds4/resnet/transferlearning_weights.hdf5\n",
            "Epoch 19/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7933 - acc: 0.3602WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 975ms/step - loss: 1.7932 - acc: 0.3603 - val_loss: 3.0803 - val_acc: 0.1889\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.32386\n",
            "Epoch 20/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8415 - acc: 0.3650WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 1s/step - loss: 1.8413 - acc: 0.3649 - val_loss: 8.4313 - val_acc: 0.0852\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.32386\n",
            "Epoch 21/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8373 - acc: 0.3733WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 976ms/step - loss: 1.8371 - acc: 0.3733 - val_loss: 8.6270 - val_acc: 0.2443\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.32386\n",
            "Epoch 22/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8074 - acc: 0.3657WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 975ms/step - loss: 1.8074 - acc: 0.3658 - val_loss: 2.1582 - val_acc: 0.3253\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.32386 to 0.32528, saving model to /content/drive/MyDrive/experimentos/v2.0-exp7-ds4/resnet/transferlearning_weights.hdf5\n",
            "Epoch 23/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8173 - acc: 0.3571WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 93s 975ms/step - loss: 1.8170 - acc: 0.3573 - val_loss: 8.5439 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.32528\n",
            "Epoch 24/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7826 - acc: 0.3809WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 973ms/step - loss: 1.7827 - acc: 0.3809 - val_loss: 2.6740 - val_acc: 0.1818\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.32528\n",
            "Epoch 25/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7730 - acc: 0.3821WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 969ms/step - loss: 1.7731 - acc: 0.3820 - val_loss: 25.1869 - val_acc: 0.2415\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.32528\n",
            "Epoch 26/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8171 - acc: 0.3772WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 93s 981ms/step - loss: 1.8167 - acc: 0.3773 - val_loss: 3.9783 - val_acc: 0.1293\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.32528\n",
            "Epoch 27/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.8051 - acc: 0.3776WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 974ms/step - loss: 1.8049 - acc: 0.3777 - val_loss: 2.1689 - val_acc: 0.3707\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.32528 to 0.37074, saving model to /content/drive/MyDrive/experimentos/v2.0-exp7-ds4/resnet/transferlearning_weights.hdf5\n",
            "Epoch 28/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7735 - acc: 0.3873WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 987ms/step - loss: 1.7734 - acc: 0.3873 - val_loss: 5.6170 - val_acc: 0.0909\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.37074\n",
            "Epoch 29/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7588 - acc: 0.3849WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 971ms/step - loss: 1.7588 - acc: 0.3849 - val_loss: 1.9221 - val_acc: 0.3381\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.37074\n",
            "Epoch 30/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7960 - acc: 0.3786WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 977ms/step - loss: 1.7957 - acc: 0.3786 - val_loss: 11.4747 - val_acc: 0.0824\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.37074\n",
            "Epoch 31/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7696 - acc: 0.3896WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 972ms/step - loss: 1.7696 - acc: 0.3896 - val_loss: 9.0656 - val_acc: 0.2528\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.37074\n",
            "Epoch 32/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "87/88 [============================>.] - ETA: 0s - loss: 1.7393 - acc: 0.3896WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 973ms/step - loss: 1.7400 - acc: 0.3895 - val_loss: 10.0845 - val_acc: 0.1562\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.37074\n",
            "Epoch 33/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7355 - acc: 0.4012WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 968ms/step - loss: 1.7358 - acc: 0.4011 - val_loss: 6.2671 - val_acc: 0.0881\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.37074\n",
            "Epoch 34/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7035 - acc: 0.4107WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 978ms/step - loss: 1.7041 - acc: 0.4106 - val_loss: 10.8881 - val_acc: 0.0810\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.37074\n",
            "Epoch 35/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7646 - acc: 0.3825WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 972ms/step - loss: 1.7645 - acc: 0.3826 - val_loss: 6.2208 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.37074\n",
            "Epoch 36/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7729 - acc: 0.3918WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 983ms/step - loss: 1.7726 - acc: 0.3919 - val_loss: 2.9408 - val_acc: 0.1733\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.37074\n",
            "Epoch 37/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7439 - acc: 0.4025WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}